So a big uncertainty that has been looming over the project so far has been the question of what exactly will control the image manipulation, and what exactly I am aiming to achieve. I was undecided whether I was just going to experiment and pick what worked best, or whether I aimed to achieve one specific experience (i.e. a synaesthesia simulator), or whether user interaction would control the manipulation.

I've been thinking a lot, and I've just had a brainwave (possibly spurred on by the excitement of being featured today, which is awesome!). My original intention was just to stream the live feed as seen by the viewer, but after having a more in depth look at the hangouts API - which is a lot more powerful than I had presumed -  I thought: why not integrate user participation into the experience and incorporate this into the hangouts live stream? This way, instead of me dictating the experience of the user in the gallery space, people online can control the experience! (Which has been inspired in a large part by [Google's web lab experiment with the science museum](http://www.chromeexperiments.com/detail/web-lab/?f)). 

To do this, I will create a series of triggers and responses that are interchangeable and stackable. Then anyone can login to the Mediated Perception website and choose a set of parameters, which will then be added to a queue of experiences. Every time a new visitor at the exhibition puts the headset on, it will execute a new experience that has been chosen online. This will be live streamed online, and the online user who selected it will be given a notification just before their experience is aired (perhaps it will even be saved and stored online).

![image1](../project_images/controlanimation.gif)
