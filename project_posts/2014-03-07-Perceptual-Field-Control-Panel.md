So a big uncertainty which has been looming over the project so far has been the question of what exactly was going to control the image manipulation and what exactly I was aiming to achieve. Whether I was going to experiment and pick what worked myself, or whether I work aim to achieve one specific experience (i.e. a synaesthesia simulator), or wether user interaction would control the manipulation. 

I've been thinking alot, and I've just had a brainwave (possibly spurned on by the excitment of being featured today, which is awesome!). My original intention was just to stream the live feed seen by the viewer, but after having a more in depth look at the hangouts API (which is alot more poweful than I had presumed). Why not integrate user participation into the experience and intrate this into the hangouts live steam? 

So instead of me choosing the experience of the user in the gallery space, why not let people on the internet control the experience? I will create a series of triggers and responses that are interchangeable and stackable, then anyone can login to the mediated perception website, choose a set of parameters, then it will be added to a queue of experiences. And everytime a new visitor to the exhibition puts the headset on, it will excecute a new experience that has been chosen online. Which will be livestreamed online, and the user online will be given a notification just before there experience is going to be aired (perhaps it will even be saved and stored online). 

![image1](../project_images/controlanimation.gif)
